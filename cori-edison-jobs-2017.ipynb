{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the size of big jobs at NERSC\n",
    "\n",
    "This notebook demonstrates how to quantify what fraction of a compute platform is typically used by \"big\" jobs.  We define \"big\" jobs as those which use up significant resources on the system, where resources could be\n",
    "\n",
    "1. total fraction of available compute nodes\n",
    "2. total fraction of available CPU hours delivered in a given allocation year\n",
    "\n",
    "In this work we chose definition #2 so that we do not bias ourselves towards very large but very short debug jobs.  We select the jobs that consumed the most cycles, then examine the job size distribution within this collection of big jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "import matplotlib.pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAX_EDISON = 5586.0\n",
    "MAX_CORI_KNL = 9688.0\n",
    "MAX_CORI_HSW = 2388.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pandas.read_csv('cori-edison-jobs-2017.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['nodehrs'] = df['numnodes'] * df['wallclock'] / 3600.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of cycles by job class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Millions of cycles delivered per superclass\"\n",
    "grouped = df.groupby(by=['hostname', 'superclass'])\n",
    "grouped['nodehrs'].sum() / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Percent of delivered cycles per superclass\"\n",
    "grouped['nodehrs'].sum().groupby(level=0).transform(lambda x: 100.0 * x / x.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the below cori-hsw definition is WRONG\n",
    "df_by_system = {\n",
    "    'cori-knl': df[(df['hostname'] == 'cori') \n",
    "                    & ((df['superclass'].str.startswith('knl'))\n",
    "                    |  ((df['numnodes'] > MAX_CORI_HSW)\n",
    "                    &   (df['numnodes'] <= MAX_CORI_KNL)))].copy(),\n",
    "    'cori-hsw': df[\n",
    "                (df['hostname'] == 'cori') \n",
    "                & (~(df['superclass'].str.startswith('knl'))\n",
    "                &   (df['superclass'] != 'system')\n",
    "                &   (df['superclass'] != 'benchmark')\n",
    "                &   ((df['superclass'] != 'special') | (df['numnodes'] <= MAX_CORI_HSW)))\n",
    "                  ].copy(),\n",
    "    'edison': df[df['hostname'] == 'edison'].copy(),\n",
    "}\n",
    "\n",
    "del df ### collect garbage now that the dataframe has been split out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for sys in df_by_system.keys():\n",
    "    df_by_system[sys]['fraction_total'] = df_by_system[sys]['nodehrs'] / df_by_system[sys]['nodehrs'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job Count Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_hist(df, key, title=\"\"):\n",
    "    fig, ax = matplotlib.pyplot.subplots(figsize=(12,4))\n",
    "    df[key].hist(ax=ax, bins=512)\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.set_ylabel(\"Job Count\")\n",
    "    ax.set_xlabel(key)\n",
    "    ax.set_xlim((1, 10000))\n",
    "    fig.suptitle(title)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for sys in sorted(df_by_system.keys()):\n",
    "    df = df_by_system[sys]\n",
    "    print \"%s summary\" % sys\n",
    "    print df[['numnodes', 'nodehrs']].describe()\n",
    "    plot_hist(df, 'nodehrs', sys.title())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cumulative Distribution of Cycles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions to calculate and analyze CDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_csum(df, sys, by, norm=True):\n",
    "    \"\"\"Calculate the cumulative sum for \"\"\"\n",
    "    df_working = df.sort_values(by=by, ascending=False)\n",
    "\n",
    "    csum = 0.0\n",
    "    csum_y = []\n",
    "    for value in df_working[by]:\n",
    "        csum += value\n",
    "        csum_y.append(csum)\n",
    "\n",
    "    # Normalize cumulative sum so we get a fraction\n",
    "    if norm:\n",
    "        csum_y /= csum\n",
    "    return csum_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_csum(csum_y):\n",
    "    \"\"\"Plot the cumulative sum function\"\"\"\n",
    "    fig, ax = matplotlib.pyplot.subplots(figsize=(12,4))\n",
    "\n",
    "    ax.plot(csum_y)\n",
    "    # ax.set_yscale(\"log\")\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_ylim((0.01,1.0))\n",
    "    ax.set_yticks(numpy.arange(0,1,0.1))\n",
    "    ax.set_xlabel(\"Number of Jobs\")\n",
    "    ax.set_ylabel(\"Fraction of 2017 cycles consumed\")\n",
    "    ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def define_bigjobs(df, sys, x):\n",
    "    \"\"\"Define what a 'big job' is\"\"\"\n",
    "    tot_nodes = {\n",
    "        'edison': MAX_EDISON,\n",
    "        'cori-knl': MAX_CORI_KNL,\n",
    "        'cori-hsw': MAX_CORI_HSW,\n",
    "    }\n",
    "    csum_y = calculate_csum(df, sys, by='nodehrs', norm=True)\n",
    "\n",
    "    jobcount = numpy.interp(x=x,\n",
    "                            fp=numpy.arange(len(csum_y)),\n",
    "                            xp=csum_y)\n",
    "    jobcount = int(jobcount)\n",
    "    \n",
    "    df_sorted_by_nodehrs = df.sort_values(by='nodehrs', ascending=False)\n",
    "    summary = (df_sorted_by_nodehrs.iloc[0:jobcount]['numnodes'] / tot_nodes[sys]).describe()\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def summarize_summary(summary, sys):\n",
    "    col_descriptions = {\n",
    "        \"mean\": \"average\",\n",
    "        \"min\": \"smallest\",\n",
    "        \"25%\": \"25th percentile\",\n",
    "        \"50%\": \"median\",\n",
    "        \"75%\": \"75th percentile\",\n",
    "        \"max\": \"biggest\",\n",
    "    }\n",
    "    print 'N is %d jobs' % summary['count']\n",
    "    for stat, descr in col_descriptions.iteritems():\n",
    "        str_begin = '%s \"big job\" used' % descr\n",
    "        print \"%30s %5.1f%% of %s\\'s nodes\" % (str_begin, 100.0 * summary[stat], sys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run some actual analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = 0.50\n",
    "sys = 'cori-knl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_csum(calculate_csum(df_by_system[sys], sys, by='nodehrs', norm=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at all available systems now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Assuming \"big\" jobs are the N largest jobs who, in total, used up %d%% of the total cycles delivered:' % (100.0*x)\n",
    "for sys in df_by_system.keys():\n",
    "    print \"\\n===== %s =====\" % sys\n",
    "    summary = define_bigjobs(df_by_system[sys], sys, x)\n",
    "    summarize_summary(summary, sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
